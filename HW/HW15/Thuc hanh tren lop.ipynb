{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thực hành Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bài này, ta sẽ thực hành cài đặt Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cài đặt và import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WdMvH4x9swj",
    "outputId": "5c077793-40fe-445e-d2e1-1434c810775e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\python.exe\n",
      "C:\\Users\\Admin\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4ObXVBqjGby",
    "outputId": "6ea62d08-41e6-41e4-9960-80b8de42c77d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\anaconda\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: dill in d:\\anaconda\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\anaconda\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\anaconda\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\anaconda\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\anaconda\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\anaconda\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\anaconda\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\anaconda\\lib\\site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\anaconda\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in d:\\anaconda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\anaconda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\anaconda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\anaconda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
      "Requirement already satisfied: wrapt in d:\\anaconda\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\anaconda\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Requirement already satisfied: torchtext in d:\\anaconda\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from torchtext) (4.67.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from torchtext) (2.32.4)\n",
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (from torchtext) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchtext) (2.2.6)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from torchtext) (1.17.0)\n",
      "Requirement already satisfied: sentencepiece in d:\\anaconda\\lib\\site-packages (from torchtext) (0.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (2025.10.5)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch->torchtext) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\lib\\site-packages (from torch->torchtext) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda\\lib\\site-packages (from torch->torchtext) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\anaconda\\lib\\site-packages (from torch->torchtext) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch->torchtext) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\anaconda\\lib\\site-packages (from torch->torchtext) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch->torchtext) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch->torchtext) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch->torchtext) (3.0.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spacy dill\n",
    "!pip3 install torchtext\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US4j_5D69swl",
    "outputId": "67905392-c2fa-4753-f48a-35c0d9a9f192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OaNedrhUjGb0"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchtext\n",
    "import copy\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6462QxLjGb0"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cài đặt từng module của Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DosoYt1YjGb0"
   },
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBaPrBG0ubI-"
   },
   "source": [
    "**Position Embedding Class**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MJs0_6GwjGb1"
   },
   "outputs": [],
   "source": [
    "# Positional encoding\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # create a constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, dim)\n",
    "\n",
    "        for i in range(max_seq_len):\n",
    "            for j in range(0, dim, 2):\n",
    "                pe[i, j] = math.sin(i / (1000 ** (2*j/dim)))\n",
    "                pe[i, j+1] = math.cos(i / (1000 ** (2*(j+1)/dim)))\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x *math.sqrt(self.dim)\n",
    "        # add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:, :seq_len], requires_grad=False).to(device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F25tcm9tu1ce"
   },
   "source": [
    "**Multi Head Attention**: We first start with implementing attention function\n",
    "\n",
    "Attention of $q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wJEKoan4jGb2"
   },
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    Args:\n",
    "        q: (batch, heads, seq_len_q, d_k)\n",
    "        k: (batch, heads, seq_len_k, d_k)\n",
    "        v: (batch, heads, seq_len_k, d_k)\n",
    "        mask: \n",
    "            - src_mask: (batch, 1, seq_len_k)\n",
    "            - trg_mask: (batch, seq_len_q, seq_len_q)\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, heads, seq_len_q, d_k)\n",
    "    \"\"\"\n",
    "    # Calculate attention scores\n",
    "    # scores: (batch, heads, seq_len_q, seq_len_k)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        # Expand mask dimensions to match scores\n",
    "        # mask: (batch, 1, seq_len) -> (batch, 1, 1, seq_len)\n",
    "        # or mask: (batch, seq_len, seq_len) -> (batch, 1, seq_len, seq_len)\n",
    "        if mask.dim() == 3:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        # Apply mask\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4vvVcNXBjGb1"
   },
   "outputs": [],
   "source": [
    "# Multi-headed attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_head = dim//heads\n",
    "        self.h = heads\n",
    "        self.q_linear = nn.Linear(dim, dim)\n",
    "        self.k_linear = nn.Linear(dim, dim)\n",
    "        self.v_linear = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.dim_head)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.dim_head)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.dim_head)\n",
    "        # transpose to get dimensions bs * h * sl * dim\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        # calculate attention using the function we will define next\n",
    "        scores = attention(q, k, v, self.dim, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.dim)\n",
    "        output = self.out(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2Cy0Xt9QjGb2"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "v7UTrblYjGb2"
   },
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1uYXmKKsjGb2"
   },
   "outputs": [],
   "source": [
    "# build an encoder layer with one multi-head attention layer and one \n",
    "# feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2, x2, x2, mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8AWL51G_jGb3"
   },
   "outputs": [],
   "source": [
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model).cuda()\n",
    "    \n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x =  x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mQAOcVwqjGb3"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TtVVMjazjGb3"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output# we don't perform softmax on the output as this will be handled \n",
    "# automatically by our loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chuẩn bị và tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZY3841jbjGb4"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "class tokenize(object):\n",
    "    \n",
    "    def __init__(self, lang):\n",
    "        self.nlp = spacy.load(lang)\n",
    "            \n",
    "    def tokenizer(self, sentence):\n",
    "        sentence = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLDB_d_mjGb4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Creating batch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlegacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "# # Creating batch\n",
    "# from torchtext.legacy import data\n",
    "# import numpy as np\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# def nopeak_mask(size, opt):\n",
    "#     np_mask = np.triu(np.ones((1, size, size)),k=1).astype('uint8')\n",
    "#     np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "#     np_mask = np_mask.to(device)\n",
    "#     return np_mask\n",
    "\n",
    "# def create_masks(src, trg, opt):\n",
    "    \n",
    "#     src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
    "\n",
    "#     if trg is not None:\n",
    "#         trg.to(device)\n",
    "#         trg_mask = (trg != opt.trg_pad).unsqueeze(-2).to(device)\n",
    "#         size = trg.size(1) # get seq_len for matrix\n",
    "#         np_mask = nopeak_mask(size, opt)\n",
    "#         trg_mask = trg_mask & np_mask\n",
    "        \n",
    "#     else:\n",
    "#         trg_mask = None\n",
    "#     return src_mask, trg_mask\n",
    "\n",
    "# # patch on Torchtext's batching process that makes it more efficient\n",
    "# # from http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n",
    "\n",
    "# class MyIterator(data.Iterator):\n",
    "#     def create_batches(self):\n",
    "#         if self.train:\n",
    "#             def pool(d, random_shuffler):\n",
    "#                 for p in data.batch(d, self.batch_size * 100):\n",
    "#                     p_batch = data.batch(\n",
    "#                         sorted(p, key=self.sort_key),\n",
    "#                         self.batch_size, self.batch_size_fn)\n",
    "#                     for b in random_shuffler(list(p_batch)):\n",
    "#                         yield b\n",
    "#             self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "#         else:\n",
    "#             self.batches = []\n",
    "#             for b in data.batch(self.data(), self.batch_size,\n",
    "#                                           self.batch_size_fn):\n",
    "#                 self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "# global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "# def batch_size_fn(new, count, sofar):\n",
    "#     \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "#     global max_src_in_batch, max_tgt_in_batch\n",
    "#     if count == 1:\n",
    "#         max_src_in_batch = 0\n",
    "#         max_tgt_in_batch = 0\n",
    "#     max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "#     max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "#     src_elements = count * max_src_in_batch\n",
    "#     tgt_elements = count * max_tgt_in_batch\n",
    "#     return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Assuming you have device defined somewhere\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def nopeak_mask(size, opt):\n",
    "    \"\"\"Create mask to prevent attention to future positions\"\"\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    np_mask = torch.from_numpy(np_mask) == 0\n",
    "    np_mask = np_mask.to(device)\n",
    "    return np_mask\n",
    "\n",
    "\n",
    "def create_masks(src, trg, opt):\n",
    "    \"\"\"Create source and target masks\"\"\"\n",
    "    src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg = trg.to(device)\n",
    "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2).to(device)\n",
    "        size = trg.size(1)  # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt)\n",
    "        trg_mask = trg_mask & np_mask\n",
    "    else:\n",
    "        trg_mask = None\n",
    "    \n",
    "    return src_mask, trg_mask\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, trg_data):\n",
    "        self.src_data = src_data\n",
    "        self.trg_data = trg_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'src': torch.tensor(self.src_data[idx], dtype=torch.long),\n",
    "            'trg': torch.tensor(self.trg_data[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch, src_pad_idx=0, trg_pad_idx=0):\n",
    "   \n",
    "    src_batch = [item['src'] for item in batch]\n",
    "    trg_batch = [item['trg'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=src_pad_idx)\n",
    "    trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=trg_pad_idx)\n",
    "    \n",
    "    return src_padded, trg_padded\n",
    "\n",
    "\n",
    "class MyIterator:\n",
    "    def __init__(self, dataset, batch_size, device, train=True, \n",
    "                 shuffle=True, sort_key=None):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.train = train\n",
    "        self.shuffle = shuffle\n",
    "        self.sort_key = sort_key if sort_key else (lambda x: len(x['src']))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.train:\n",
    "            # Create pool of batches for training\n",
    "            indices = list(range(len(self.dataset)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "            \n",
    "            # Pool batches of size batch_size * 100, sort them, then create smaller batches\n",
    "            pool_size = self.batch_size * 100\n",
    "            for i in range(0, len(indices), pool_size):\n",
    "                pool_indices = indices[i:i + pool_size]\n",
    "                pool_data = [self.dataset[idx] for idx in pool_indices]\n",
    "                \n",
    "                # Sort pool by source length\n",
    "                pool_data_sorted = sorted(pool_data, key=self.sort_key)\n",
    "                \n",
    "                # Create batches from sorted pool\n",
    "                for j in range(0, len(pool_data_sorted), self.batch_size):\n",
    "                    batch_data = pool_data_sorted[j:j + self.batch_size]\n",
    "                    if len(batch_data) > 0:\n",
    "                        yield self._create_batch(batch_data)\n",
    "        else:\n",
    "            # For validation/test, just create batches in order\n",
    "            for i in range(0, len(self.dataset), self.batch_size):\n",
    "                batch_indices = list(range(i, min(i + self.batch_size, len(self.dataset))))\n",
    "                batch_data = [self.dataset[idx] for idx in batch_indices]\n",
    "                batch_data_sorted = sorted(batch_data, key=self.sort_key)\n",
    "                yield self._create_batch(batch_data_sorted)\n",
    "    \n",
    "    def _create_batch(self, batch_data):\n",
    "        \"\"\"Create a batch from list of data\"\"\"\n",
    "        src_batch = [item['src'] for item in batch_data]\n",
    "        trg_batch = [item['trg'] for item in batch_data]\n",
    "        \n",
    "        src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "        trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
    "        \n",
    "        return src_padded.to(self.device), trg_padded.to(self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "def batch_size_fn(batch_data, max_src_len=0, max_trg_len=0):\n",
    "    count = len(batch_data)\n",
    "    \n",
    "    for item in batch_data:\n",
    "        max_src_len = max(max_src_len, len(item['src']))\n",
    "        max_trg_len = max(max_trg_len, len(item['trg']) + 2)  # +2 for BOS and EOS\n",
    "    \n",
    "    src_elements = count * max_src_len\n",
    "    trg_elements = count * max_trg_len\n",
    "    \n",
    "    return max(src_elements, trg_elements)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "T0jx8nh0jGb5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter, OrderedDict\n",
    "import os\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ==================== VOCABULARY CLASS ====================\n",
    "class Vocabulary:\n",
    "    \"\"\"Custom vocabulary class to replace torchtext.vocab\"\"\"\n",
    "    def __init__(self, counter=None, specials=['<pad>', '<unk>'], min_freq=1):\n",
    "        self.itos = []  # index to string\n",
    "        self.stoi = {}  # string to index\n",
    "        self.freqs = counter if counter else Counter()\n",
    "        \n",
    "        # Add special tokens first\n",
    "        for token in specials:\n",
    "            self.add_token(token)\n",
    "        \n",
    "        # Add other tokens based on frequency\n",
    "        if counter:\n",
    "            for token, freq in counter.items():\n",
    "                if freq >= min_freq and token not in self.stoi:\n",
    "                    self.add_token(token)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token not in self.stoi:\n",
    "            idx = len(self.itos)\n",
    "            self.itos.append(token)\n",
    "            self.stoi[token] = idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.stoi.get('<unk>', 1))\n",
    "\n",
    "\n",
    "class Field:\n",
    "    \"\"\"Custom Field class to replace torchtext.legacy.data.Field\"\"\"\n",
    "    def __init__(self, tokenize=None, lower=False, init_token=None, eos_token=None, \n",
    "                 pad_token='<pad>', unk_token='<unk>'):\n",
    "        self.tokenize = tokenize if tokenize else str.split\n",
    "        self.lower = lower\n",
    "        self.init_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.vocab = None\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Tokenize and lowercase if needed\"\"\"\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # Add special tokens\n",
    "        if self.init_token:\n",
    "            tokens = [self.init_token] + tokens\n",
    "        if self.eos_token:\n",
    "            tokens = tokens + [self.eos_token]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, dataset, min_freq=1):\n",
    "        \"\"\"Build vocabulary from dataset\"\"\"\n",
    "        counter = Counter()\n",
    "        for example in dataset:\n",
    "            tokens = self.preprocess(example)\n",
    "            counter.update(tokens)\n",
    "        \n",
    "        specials = [self.pad_token, self.unk_token]\n",
    "        if self.init_token:\n",
    "            specials.append(self.init_token)\n",
    "        if self.eos_token:\n",
    "            specials.append(self.eos_token)\n",
    "        \n",
    "        self.vocab = Vocabulary(counter, specials=specials, min_freq=min_freq)\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        \"\"\"Convert text to indices\"\"\"\n",
    "        tokens = self.preprocess(text)\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "\n",
    "\n",
    "# ==================== TOKENIZER ====================\n",
    "class Tokenizer:\n",
    "    \"\"\"Wrapper for spacy tokenizer\"\"\"\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        try:\n",
    "            self.nlp = spacy.load(lang)\n",
    "        except:\n",
    "            print(f\"Spacy model '{lang}' not found. Installing...\")\n",
    "            os.system(f'python -m spacy download {lang}')\n",
    "            self.nlp = spacy.load(lang)\n",
    "    \n",
    "    def tokenizer(self, text):\n",
    "        return [tok.text for tok in self.nlp.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize(lang):\n",
    "    \"\"\"Create tokenizer for given language\"\"\"\n",
    "    lang_dict = {\n",
    "        'en': 'en_core_web_sm',\n",
    "        'fr': 'fr_core_news_sm',\n",
    "        'de': 'de_core_news_sm',\n",
    "        'es': 'es_core_news_sm',\n",
    "        'pt': 'pt_core_news_sm',\n",
    "        'it': 'it_core_news_sm',\n",
    "        'nl': 'nl_core_news_sm'\n",
    "    }\n",
    "    \n",
    "    lang_code = lang[0:2]\n",
    "    if lang_code in lang_dict:\n",
    "        return Tokenizer(lang_dict[lang_code])\n",
    "    else:\n",
    "        raise ValueError(f\"Language {lang} not supported\")\n",
    "\n",
    "\n",
    "# ==================== DATASET ====================\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for translation tasks\"\"\"\n",
    "    def __init__(self, src_data, trg_data, src_field, trg_field):\n",
    "        self.src_data = src_data\n",
    "        self.trg_data = trg_data\n",
    "        self.src_field = src_field\n",
    "        self.trg_field = trg_field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_data[idx]\n",
    "        trg_text = self.trg_data[idx]\n",
    "        \n",
    "        # Numericalize\n",
    "        src_indices = self.src_field.numericalize(src_text)\n",
    "        trg_indices = self.trg_field.numericalize(trg_text)\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
    "            'trg': torch.tensor(trg_indices, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== ITERATOR ====================\n",
    "class MyIterator:\n",
    "    \"\"\"Custom iterator with batch pooling and sorting\"\"\"\n",
    "    def __init__(self, dataset, batch_size, device, train=True, \n",
    "                 shuffle=True, sort_key=None, batch_size_fn=None, repeat=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.train = train\n",
    "        self.shuffle = shuffle\n",
    "        self.sort_key = sort_key if sort_key else (lambda x: len(x['src']))\n",
    "        self.batch_size_fn = batch_size_fn\n",
    "        self.repeat = repeat\n",
    "        \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.train:\n",
    "                # Create pool of batches for training\n",
    "                indices = list(range(len(self.dataset)))\n",
    "                if self.shuffle:\n",
    "                    np.random.shuffle(indices)\n",
    "                \n",
    "                # Pool batches of size batch_size * 100, sort them, then create smaller batches\n",
    "                pool_size = self.batch_size * 100\n",
    "                for i in range(0, len(indices), pool_size):\n",
    "                    pool_indices = indices[i:i + pool_size]\n",
    "                    pool_data = [self.dataset[idx] for idx in pool_indices]\n",
    "                    \n",
    "                    # Sort pool by source and target length\n",
    "                    pool_data_sorted = sorted(pool_data, key=self.sort_key)\n",
    "                    \n",
    "                    # Create batches from sorted pool\n",
    "                    for j in range(0, len(pool_data_sorted), self.batch_size):\n",
    "                        batch_data = pool_data_sorted[j:j + self.batch_size]\n",
    "                        if len(batch_data) > 0:\n",
    "                            yield self._create_batch(batch_data)\n",
    "            else:\n",
    "                # For validation/test, just create batches in order\n",
    "                for i in range(0, len(self.dataset), self.batch_size):\n",
    "                    batch_indices = list(range(i, min(i + self.batch_size, len(self.dataset))))\n",
    "                    batch_data = [self.dataset[idx] for idx in batch_indices]\n",
    "                    batch_data_sorted = sorted(batch_data, key=self.sort_key)\n",
    "                    yield self._create_batch(batch_data_sorted)\n",
    "            \n",
    "            if not self.repeat:\n",
    "                break\n",
    "    \n",
    "    def _create_batch(self, batch_data):\n",
    "        \"\"\"Create a batch from list of data\"\"\"\n",
    "        src_batch = [item['src'] for item in batch_data]\n",
    "        trg_batch = [item['trg'] for item in batch_data]\n",
    "        \n",
    "        src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "        trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # Create simple batch object\n",
    "        class Batch:\n",
    "            def __init__(self, src, trg):\n",
    "                self.src = src\n",
    "                self.trg = trg\n",
    "        \n",
    "        return Batch(src_padded.to(self.device), trg_padded.to(self.device))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "# ==================== MASKS ====================\n",
    "def nopeak_mask(size, opt):\n",
    "    \"\"\"Create causal mask to prevent attention to future positions\"\"\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    np_mask = torch.from_numpy(np_mask) == 0\n",
    "    np_mask = np_mask.to(device)\n",
    "    return np_mask\n",
    "\n",
    "\n",
    "def create_masks(src, trg, opt):\n",
    "    \"\"\"Create masks for Transformer\"\"\"\n",
    "    src_mask = (src != opt.src_pad).unsqueeze(1)\n",
    "    \n",
    "    if trg is not None:\n",
    "        trg_pad_mask = (trg != opt.trg_pad).unsqueeze(1)\n",
    "        size = trg.size(1)\n",
    "        nopeak = nopeak_mask(size, opt)\n",
    "        trg_mask = trg_pad_mask & nopeak\n",
    "    else:\n",
    "        trg_mask = None\n",
    "    \n",
    "    return src_mask, trg_mask\n",
    "\n",
    "\n",
    "# ==================== BATCH SIZE FUNCTION ====================\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"\"\"Keep augmenting batch and calculate total number of tokens + padding.\"\"\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch, len(new['src']))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch, len(new['trg']))\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "\n",
    "# ==================== DATA LOADING FUNCTIONS ====================\n",
    "def read_data(opt):\n",
    "    \"\"\"Read source and target data files\"\"\"\n",
    "    if opt.src_data is not None:\n",
    "        try:\n",
    "            with open(opt.src_data, 'r', encoding='utf-8') as f:\n",
    "                opt.src_data = [line.strip() for line in f if line.strip()]\n",
    "            print(f\"✓ Loaded {len(opt.src_data)} source sentences\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Cannot read '{opt.src_data}': {e}\")\n",
    "            quit()\n",
    "    \n",
    "    if opt.trg_data is not None:\n",
    "        try:\n",
    "            with open(opt.trg_data, 'r', encoding='utf-8') as f:\n",
    "                opt.trg_data = [line.strip() for line in f if line.strip()]\n",
    "            print(f\"✓ Loaded {len(opt.trg_data)} target sentences\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Cannot read '{opt.trg_data}': {e}\")\n",
    "            quit()\n",
    "    \n",
    "    # Check and fix length mismatch\n",
    "    if len(opt.src_data) != len(opt.trg_data):\n",
    "        min_len = min(len(opt.src_data), len(opt.trg_data))\n",
    "        print(f\"⚠ WARNING: Source ({len(opt.src_data)}) and target ({len(opt.trg_data)}) lengths differ!\")\n",
    "        print(f\"⚠ Truncating to {min_len} parallel sentences.\")\n",
    "        opt.src_data = opt.src_data[:min_len]\n",
    "        opt.trg_data = opt.trg_data[:min_len]\n",
    "\n",
    "\n",
    "def create_fields(opt):\n",
    "    \"\"\"Create source and target fields with tokenizers\"\"\"\n",
    "    spacy_langs = ['en', 'fr', 'de', 'es', 'pt', 'it', 'nl']\n",
    "    src_lang = opt.src_lang[0:2]\n",
    "    trg_lang = opt.trg_lang[0:2]\n",
    "    \n",
    "    if src_lang not in spacy_langs:\n",
    "        print(f'ERROR: Invalid src language: {opt.src_lang}')\n",
    "        print(f'Supported: {spacy_langs}')\n",
    "        quit()\n",
    "    if trg_lang not in spacy_langs:\n",
    "        print(f'ERROR: Invalid trg language: {opt.trg_lang}')\n",
    "        print(f'Supported: {spacy_langs}')\n",
    "        quit()\n",
    "    \n",
    "    print(\"Loading spacy tokenizers...\")\n",
    "    \n",
    "    t_src = tokenize(opt.src_lang)\n",
    "    t_trg = tokenize(opt.trg_lang)\n",
    "    \n",
    "    TRG = Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "    SRC = Field(lower=True, tokenize=t_src.tokenizer)\n",
    "\n",
    "    return SRC, TRG\n",
    "\n",
    "\n",
    "def create_dataset(opt, SRC, TRG):\n",
    "    \"\"\"Create dataset and iterator\"\"\"\n",
    "    print(\"\\nCreating dataset and iterator...\")\n",
    "    \n",
    "    # Verify lengths match\n",
    "    assert len(opt.src_data) == len(opt.trg_data), \\\n",
    "        f\"Data length mismatch: src={len(opt.src_data)}, trg={len(opt.trg_data)}\"\n",
    "    \n",
    "    raw_data = {'src': opt.src_data, 'trg': opt.trg_data}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "    \n",
    "    print(f\"Total parallel sentences: {len(df)}\")\n",
    "    \n",
    "    # Filter by max string length\n",
    "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
    "    df = df.loc[mask]\n",
    "    \n",
    "    print(f\"After filtering (max_strlen={opt.max_strlen}): {len(df)}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"ERROR: No data left after filtering! Try increasing max_strlen.\")\n",
    "        quit()\n",
    "    \n",
    "    src_data = df['src'].tolist()\n",
    "    trg_data = df['trg'].tolist()\n",
    "    \n",
    "    # Build vocabularies\n",
    "    print(\"Building vocabularies...\")\n",
    "    SRC.build_vocab(src_data)\n",
    "    TRG.build_vocab(trg_data)\n",
    "    \n",
    "    print(f\"✓ Source vocab size: {len(SRC.vocab)}\")\n",
    "    print(f\"✓ Target vocab size: {len(TRG.vocab)}\")\n",
    "    \n",
    "    # Set padding indices\n",
    "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TranslationDataset(src_data, trg_data, SRC, TRG)\n",
    "    \n",
    "    # Create iterator\n",
    "    train_iter = MyIterator(\n",
    "        dataset, \n",
    "        batch_size=opt.batchsize, \n",
    "        device=device,\n",
    "        repeat=False, \n",
    "        sort_key=lambda x: (len(x['src']), len(x['trg'])),\n",
    "        batch_size_fn=batch_size_fn, \n",
    "        train=True, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    opt.train_len = get_len(train_iter)\n",
    "    print(f\"✓ Number of batches: {opt.train_len}\\n\")\n",
    "\n",
    "    return train_iter\n",
    "\n",
    "\n",
    "def get_len(train):\n",
    "    \"\"\"Get length of iterator\"\"\"\n",
    "    count = 0\n",
    "    for batch in train:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cài đặt giải thuật tối ưu và huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hvXmikJB9swr"
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "class CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine annealing with restarts.\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : torch.optim.Optimizer\n",
    "    T_max : int\n",
    "        The maximum number of iterations within the first cycle.\n",
    "    eta_min : float, optional (default: 0)\n",
    "        The minimum learning rate.\n",
    "    last_epoch : int, optional (default: -1)\n",
    "        The index of the last epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 T_max: int,\n",
    "                 eta_min: float = 0.,\n",
    "                 last_epoch: int = -1,\n",
    "                 factor: float = 1.) -> None:\n",
    "        # pylint: disable=invalid-name\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.factor = factor\n",
    "        self._last_restart: int = 0\n",
    "        self._cycle_counter: int = 0\n",
    "        self._cycle_factor: float = 1.\n",
    "        self._updated_cycle_len: int = T_max\n",
    "        self._initialized: bool = False\n",
    "        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Get updated learning rate.\"\"\"\n",
    "        # HACK: We need to check if this is the first time get_lr() was called, since\n",
    "        # we want to start with step = 0, but _LRScheduler calls get_lr with\n",
    "        # last_epoch + 1 when initialized.\n",
    "        if not self._initialized:\n",
    "            self._initialized = True\n",
    "            return self.base_lrs\n",
    "\n",
    "        step = self.last_epoch + 1\n",
    "        self._cycle_counter = step - self._last_restart\n",
    "\n",
    "        lrs = [\n",
    "            (\n",
    "                self.eta_min + ((lr - self.eta_min) / 2) *\n",
    "                (\n",
    "                    np.cos(\n",
    "                        np.pi *\n",
    "                        ((self._cycle_counter) % self._updated_cycle_len) /\n",
    "                        self._updated_cycle_len\n",
    "                    ) + 1\n",
    "                )\n",
    "            ) for lr in self.base_lrs\n",
    "        ]\n",
    "\n",
    "        if self._cycle_counter % self._updated_cycle_len == 0:\n",
    "            # Adjust the cycle length.\n",
    "            self._cycle_factor *= self.factor\n",
    "            self._cycle_counter = 0\n",
    "            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n",
    "            self._last_restart = step\n",
    "\n",
    "        return lrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TRTtm5kO9swr",
    "outputId": "2c0f0c83-8b9f-4905-95fd-527d88364a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data/english.txt...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m filepath, url \u001b[38;5;129;01min\u001b[39;00m urls.items():\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDone!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\Lib\\urllib\\request.py:242\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[32m    240\u001b[39m     reporthook(blocknum, bs, size)\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m block := \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    243\u001b[39m     read += \u001b[38;5;28mlen\u001b[39m(block)\n\u001b[32m    244\u001b[39m     tfp.write(block)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Tạo thư mục data\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Download files\n",
    "urls = {\n",
    "    'data/english.txt': 'https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/english.txt',\n",
    "    'data/french.txt': 'https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/french.txt'\n",
    "}\n",
    "\n",
    "for filepath, url in urls.items():\n",
    "    print(f\"Downloading {filepath}...\")\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "    print(f\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "LSOX2OEW9swr"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model(opt, src_vocab, trg_vocab):\n",
    "    \n",
    "    assert opt.d_model % opt.heads == 0\n",
    "    assert opt.dropout < 1\n",
    "\n",
    "    model = Transformer(src_vocab, trg_vocab, opt.d_model, opt.n_layers, opt.heads)\n",
    "       \n",
    "    if opt.load_weights is not None:\n",
    "        print(\"loading pretrained weights...\")\n",
    "        model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights'))\n",
    "    else:\n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "    \n",
    "    if opt.device == 0:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q2qOSP7jGb5",
    "outputId": "8f7ea0ba-e993-4a04-f528-c2afd0eadc60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading spacy tokenizers...\n",
      "creating dataset and iterator... \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# for asking about further training use while true loop, and return\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     76\u001b[39m read_data(opt)\n\u001b[32m     77\u001b[39m SRC, TRG = create_fields(opt)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m opt.train = \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m model = get_model(opt, \u001b[38;5;28mlen\u001b[39m(SRC.vocab), \u001b[38;5;28mlen\u001b[39m(TRG.vocab)).to(device)\n\u001b[32m     81\u001b[39m opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.98\u001b[39m), eps=\u001b[32m1e-9\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 308\u001b[39m, in \u001b[36mcreate_dataset\u001b[39m\u001b[34m(opt, SRC, TRG)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcreating dataset and iterator... \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    307\u001b[39m raw_data = {\u001b[33m'\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m'\u001b[39m: [line \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m opt.src_data], \u001b[33m'\u001b[39m\u001b[33mtrg\u001b[39m\u001b[33m'\u001b[39m: [line \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m opt.trg_data]}\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# Filter by max string length\u001b[39;00m\n\u001b[32m    311\u001b[39m mask = (df[\u001b[33m'\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m'\u001b[39m].str.count(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m) < opt.max_strlen) & (df[\u001b[33m'\u001b[39m\u001b[33mtrg\u001b[39m\u001b[33m'\u001b[39m].str.count(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m) < opt.max_strlen)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\Lib\\site-packages\\pandas\\core\\frame.py:782\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    776\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    777\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    778\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    781\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:448\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    444\u001b[39m missing = arrays.isna()\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# GH10856\u001b[39;00m\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# raise ValueError if only scalars in dict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[43m~\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    450\u001b[39m     index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ANACONDA\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# def train_model(model, opt):\n",
    "    \n",
    "#     print(\"training model...\")\n",
    "#     model.train()\n",
    "#     start = time.time()\n",
    "#     if opt.checkpoint > 0:\n",
    "#         cptime = time.time()\n",
    "                 \n",
    "#     for epoch in range(opt.epochs):\n",
    "\n",
    "#         total_loss = 0\n",
    "#         print(\"   %dm: epoch %d [%s]  %d%%  loss = %s\" %\\\n",
    "#             ((time.time() - start)//60, epoch + 1, \"\".join(' '*20), 0, '...'), end='\\r')\n",
    "        \n",
    "#         if opt.checkpoint > 0:\n",
    "#             torch.save(model.state_dict(), 'weights/model_weights')\n",
    "                    \n",
    "#         for i, batch in enumerate(opt.train): \n",
    "\n",
    "#             src = batch.src.transpose(0,1).to(device)\n",
    "#             trg = batch.trg.transpose(0,1).to(device)\n",
    "#             trg_input = trg[:, :-1].to(device)\n",
    "#             src_mask, trg_mask = create_masks(src, trg_input, opt)\n",
    "#             preds = model(src, trg_input, src_mask, trg_mask)\n",
    "#             ys = trg[:, 1:].contiguous().view(-1)\n",
    "#             opt.optimizer.zero_grad()\n",
    "#             loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)\n",
    "#             loss.backward()\n",
    "#             opt.optimizer.step()\n",
    "          \n",
    "#             total_loss += loss.item()\n",
    "            \n",
    "#             if (i + 1) % opt.printevery == 0:\n",
    "#                 p = int(100 * (i + 1) / opt.train_len)\n",
    "#                 avg_loss = total_loss/opt.printevery\n",
    "#                 print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
    "#                     ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss))\n",
    "#                 total_loss = 0\n",
    "            \n",
    "#             if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n",
    "#                 torch.save(model.state_dict(), 'weights/model_weights')\n",
    "#                 cptime = time.time()\n",
    "   \n",
    "   \n",
    "#         print(\"%dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, loss = %.03f\" %\\\n",
    "#         ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_loss))\n",
    "\n",
    "# class Opt(object):\n",
    "#     pass\n",
    "        \n",
    "# def main():\n",
    "#     opt = Opt()\n",
    "#     opt.src_data = \"data/english.txt\"\n",
    "#     opt.trg_data = \"data/french.txt\"\n",
    "#     opt.src_lang = \"en_core_web_sm\"\n",
    "#     opt.trg_lang = 'fr_core_news_sm'\n",
    "#     opt.epochs = 2\n",
    "#     opt.d_model=512\n",
    "#     opt.n_layers=6\n",
    "#     opt.heads=8\n",
    "#     opt.dropout=0.1\n",
    "#     opt.batchsize=1500\n",
    "#     opt.printevery=100\n",
    "#     opt.lr=0.0001\n",
    "#     opt.max_strlen=80\n",
    "#     opt.checkpoint = 0\n",
    "#     opt.no_cuda = False\n",
    "#     opt.load_weights = None\n",
    "    \n",
    "#     opt.device = 0\n",
    "#     if opt.device == 0:\n",
    "#         assert torch.cuda.is_available()\n",
    "    \n",
    "#     read_data(opt)\n",
    "#     SRC, TRG = create_fields(opt)\n",
    "#     opt.train = create_dataset(opt, SRC, TRG)\n",
    "#     model = get_model(opt, len(SRC.vocab), len(TRG.vocab)).to(device)\n",
    "\n",
    "#     opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "#     if opt.checkpoint > 0:\n",
    "#         print(\"model weights will be saved every %d minutes and at end of epoch to directory weights/\"%(opt.checkpoint))\n",
    "    \n",
    "#     train_model(model, opt)\n",
    "\n",
    "\n",
    "#     # for asking about further training use while true loop, and return\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "✓ Loaded 53734 source sentences\n",
      "✓ Loaded 154883 target sentences\n",
      "⚠ WARNING: Source (53734) and target (154883) lengths differ!\n",
      "⚠ Truncating to 53734 parallel sentences.\n",
      "Creating fields...\n",
      "Loading spacy tokenizers...\n",
      "Creating dataset...\n",
      "\n",
      "Creating dataset and iterator...\n",
      "Total parallel sentences: 53734\n",
      "After filtering (max_strlen=80): 53734\n",
      "Building vocabularies...\n",
      "✓ Source vocab size: 6195\n",
      "✓ Target vocab size: 11409\n",
      "✓ Number of batches: 1680\n",
      "\n",
      "Source vocabulary size: 6195\n",
      "Target vocabulary size: 11409\n",
      "Training samples: 1680\n",
      "Creating model...\n",
      "Total parameters: 59,006,609\n",
      "Trainable parameters: 59,006,609\n",
      "training model...\n",
      "   3m: epoch 1 [################### ]  95%  loss = 2.932\n",
      "3m: epoch 1 [####################]  100%  loss = 2.872\n",
      "epoch 1 complete, loss = 2.872\n",
      "\n",
      "   9m: epoch 2 [################### ]  95%  loss = 2.288\n",
      "9m: epoch 2 [####################]  100%  loss = 2.292\n",
      "epoch 2 complete, loss = 2.292\n",
      "\n",
      "   19m: epoch 3 [################### ]  95%  loss = 1.835\n",
      "19m: epoch 3 [####################]  100%  loss = 1.846\n",
      "epoch 3 complete, loss = 1.846\n",
      "\n",
      "   34m: epoch 4 [################### ]  95%  loss = 1.554\n",
      "34m: epoch 4 [####################]  100%  loss = 1.572\n",
      "epoch 4 complete, loss = 1.572\n",
      "\n",
      "   52m: epoch 5 [################### ]  95%  loss = 1.379\n",
      "53m: epoch 5 [####################]  100%  loss = 1.365\n",
      "epoch 5 complete, loss = 1.365\n",
      "\n",
      "   77m: epoch 6 [################### ]  95%  loss = 1.255\n",
      "78m: epoch 6 [####################]  100%  loss = 1.242\n",
      "epoch 6 complete, loss = 1.242\n",
      "\n",
      "   95m: epoch 7 [################### ]  95%  loss = 1.123\n",
      "96m: epoch 7 [####################]  100%  loss = 1.165\n",
      "epoch 7 complete, loss = 1.165\n",
      "\n",
      "   113m: epoch 8 [################### ]  95%  loss = 1.034\n",
      "115m: epoch 8 [####################]  100%  loss = 1.044\n",
      "epoch 8 complete, loss = 1.044\n",
      "\n",
      "   130m: epoch 9 [################### ]  95%  loss = 0.990\n",
      "130m: epoch 9 [####################]  100%  loss = 0.968\n",
      "epoch 9 complete, loss = 0.968\n",
      "\n",
      "   136m: epoch 10 [################### ]  95%  loss = 0.906\n",
      "136m: epoch 10 [####################]  100%  loss = 0.937\n",
      "epoch 10 complete, loss = 0.937\n",
      "\n",
      "\n",
      "Training complete! Saving final model...\n",
      "Model saved to weights/model_weights_final\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "def train_model(model, opt):\n",
    "    \"\"\"Train the transformer model\"\"\"\n",
    "    print(\"training model...\")\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    if opt.checkpoint > 0:\n",
    "        os.makedirs('weights', exist_ok=True)\n",
    "        cptime = time.time()\n",
    "                 \n",
    "    for epoch in range(opt.epochs):\n",
    "        total_loss = 0\n",
    "        print(\"   %dm: epoch %d [%s]  %d%%  loss = %s\" %\\\n",
    "            ((time.time() - start)//60, epoch + 1, \"\".join(' '*20), 0, '...'), end='\\r')\n",
    "        \n",
    "        if opt.checkpoint > 0:\n",
    "            torch.save(model.state_dict(), 'weights/model_weights')\n",
    "        \n",
    "        # ✅ FIX: Recreate iterator for each epoch\n",
    "        for i, batch in enumerate(opt.train): \n",
    "            # ✅ FIX: Remove transpose - data is already in correct shape (batch_first=True)\n",
    "            src = batch.src.to(device)  # (batch, src_len)\n",
    "            trg = batch.trg.to(device)  # (batch, trg_len)\n",
    "            \n",
    "            # Target input: remove last token\n",
    "            trg_input = trg[:, :-1].to(device)\n",
    "            \n",
    "            # Create masks\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, opt)\n",
    "            \n",
    "            # Forward pass\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            # Target output: remove first token (<sos>)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # Backward pass\n",
    "            opt.optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)\n",
    "            loss.backward()\n",
    "            opt.optimizer.step()\n",
    "          \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % opt.printevery == 0:\n",
    "                p = int(100 * (i + 1) / opt.train_len)\n",
    "                avg_loss = total_loss / opt.printevery\n",
    "                print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
    "                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \n",
    "                     \"\".join(' '*(20-(p//5))), p, avg_loss), end='\\r')\n",
    "                total_loss = 0\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n",
    "                torch.save(model.state_dict(), 'weights/model_weights')\n",
    "                cptime = time.time()\n",
    "        \n",
    "        # Epoch complete\n",
    "        avg_loss = total_loss / max(1, opt.train_len % opt.printevery)\n",
    "        print(\"\\n%dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
    "            ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \n",
    "             \"\".join(' '*(20-(100//5))), 100, avg_loss))\n",
    "        print(\"epoch %d complete, loss = %.03f\\n\" % (epoch + 1, avg_loss))\n",
    "\n",
    "\n",
    "class Opt(object):\n",
    "    \"\"\"Options class for hyperparameters\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    opt = Opt()\n",
    "    \n",
    "    # Data paths\n",
    "    opt.src_data = \"data/english.txt\"\n",
    "    opt.trg_data = \"data/french.txt\"\n",
    "    \n",
    "    # Languages\n",
    "    opt.src_lang = \"en_core_web_sm\"\n",
    "    opt.trg_lang = 'fr_core_news_sm'\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    opt.epochs = 10\n",
    "    opt.d_model = 512\n",
    "    opt.n_layers = 6\n",
    "    opt.heads = 8\n",
    "    opt.dropout = 0.1\n",
    "    opt.batchsize = 32  # ✅ FIX: Reduced from 1500 to reasonable size\n",
    "    opt.printevery = 100\n",
    "    opt.lr = 0.0001\n",
    "    opt.max_strlen = 80  # Maximum sequence length\n",
    "    opt.checkpoint = 0  # Save every N minutes (0 = no checkpoints)\n",
    "    \n",
    "    # Device settings\n",
    "    opt.no_cuda = False\n",
    "    opt.load_weights = None\n",
    "    opt.device = 0\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if opt.device == 0:\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"WARNING: CUDA not available, using CPU\")\n",
    "            opt.device = -1\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    read_data(opt)\n",
    "    \n",
    "    # Create fields and vocabulary\n",
    "    print(\"Creating fields...\")\n",
    "    SRC, TRG = create_fields(opt)\n",
    "    \n",
    "    # Create dataset and iterator\n",
    "    print(\"Creating dataset...\")\n",
    "    opt.train = create_dataset(opt, SRC, TRG)\n",
    "    \n",
    "    # Print vocabulary sizes\n",
    "    print(f\"Source vocabulary size: {len(SRC.vocab)}\")\n",
    "    print(f\"Target vocabulary size: {len(TRG.vocab)}\")\n",
    "    print(f\"Training samples: {opt.train_len}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Create optimizer\n",
    "    opt.optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=opt.lr, \n",
    "        betas=(0.9, 0.98), \n",
    "        eps=1e-9\n",
    "    )\n",
    "    \n",
    "    # Checkpoint info\n",
    "    if opt.checkpoint > 0:\n",
    "        print(f\"Model weights will be saved every {opt.checkpoint} minutes and at end of epoch to directory weights/\")\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model, opt)\n",
    "    \n",
    "    # Save final model\n",
    "    print(\"\\nTraining complete! Saving final model...\")\n",
    "    os.makedirs('weights', exist_ok=True)\n",
    "    torch.save(model.state_dict(), 'weights/model_weights_final')\n",
    "    print(\"Model saved to weights/model_weights_final\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TransformerTut.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
